[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Posts",
    "section": "",
    "text": "Model Quantization\n\n\n\n\n\n\n\ntutorial\n\n\n\n\nHow to use Pytorch quantization API for model quantization\n\n\n\n\n\n\nApr 23, 2022\n\n\n\n\n\n\n  \n\n\n\n\nSpectral Normalization\n\n\n\n\n\n\n\ntheory\n\n\n\n\nPost about spectral norm\n\n\n\n\n\n\nMar 28, 2022\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2022-03-18-spectral-norm.html",
    "href": "posts/2022-03-18-spectral-norm.html",
    "title": "Spectral Normalization",
    "section": "",
    "text": "Spectral normalization is one of the many kinds of normalization operations that can be used in a deep learning model. In fact, it can be thought of as both a regularization and normalization technique. If you have ever asked yourself what spectral norm does or when we should use it over, say batch normalization, then read on."
  },
  {
    "objectID": "posts/2022-03-18-spectral-norm.html#norms-and-normalizations",
    "href": "posts/2022-03-18-spectral-norm.html#norms-and-normalizations",
    "title": "Spectral Normalization",
    "section": "Norms and Normalizations",
    "text": "Norms and Normalizations\nBefore we get into Spectral Normalization let’s define what a norm is. According to the Deep Learning Book, a norm is a function used to measure the “size” of vectors. Mathematically, it is a function that maps from a complex vector space to non-negative real numbers. It is denoted with single bar (\\(|.|\\)) or double bar (\\(\\|.\\|\\)). For a function to qualify as norm it has to have 3 properties:\n\n\\(|x| = 0\\), iff \\(x=0\\)\n\\(|kx| = |k|\\cdot|x|, k \\in \\mathbb{R}\\)\n\\(|x + y| \\leq |x| + |y|\\)\n\nNote that a matrix norm is not the same thing as a vector norm. I must have dozed off during this part in college, but this difference is important for the notation of spectral norm. Usually,  vector norms are denoted with \\(|.|\\) while matrix norms are denoted with \\(\\|.\\|\\). For a function to qualify as a matrix norm it needs to follow an extra property:\n\n\\(\\|AB\\| \\leq \\|A\\|\\cdot\\|B\\|\\)\n\nNow lets’s talk about normalization. In the context of deep learning, normalization usually refers to scaling and shifting of either feature maps or weight matrices. For deep models the goal of normalization is to make training easier through some mechanism like reducing internal covariate shift or reducing vanishing/exploding gradients. In deep learning we often use “norm” and “normalization” interchangeably. Batch norm or Instance norm are not “norms” according to the mathematical definition. They are “normalization” methods. That is, they scale internal feature maps to ensure that they have zero mean and unit variance.\n“Spectral norm” on the other hand is an overloaded term that can mean both a mathematical norm or a normalization method, depending on the context. In the mathematical norm sense it refers to the largest singular value of a matrix. In the normalization sense, it refers to dividing a matrix by it’s largest singular value (to get a matrix whose singular values are \\(\\leq 1\\)).\nIf you thought that’s confusing, let’s talk about the mathematical notation for spectral norm. \\(L_p\\) norms for vectors are usually denoted as \\(|x|_p\\). The very commonly used \\(L_2\\) norm, also known as Euclidean norm, is often denoted as \\(\\|x\\|_2\\) or simply \\(\\|x\\|\\). But, for matrices, the Euclidean norm, also known as the Frobenius norm, is denoted with \\(\\|A\\|_F\\), while \\(\\|A\\|_2\\) denotes the spectral norm. Not confusing at all!\nBut seriously, there is a reason for this notation. \\(L_p\\) norms for matrices is yet another overloaded term that can mean 3 different things(1, 2 and 3). In deep learning when we calculate \\(L_p\\) norms of n-dimensional tensors, we implicitly refer to the vector norms calculated after flattening the tensor. Wikipedia refers to this as entry-wise matrix norms and denotes it as \\(L_{p,p}\\) norm for matrices (maybe \\(L_{p,p...\\text{n times}}\\) for \\(n\\)-dimensional tensors?). Under this more precise definition Euclidean norm is the \\(L_{2,2}\\) matrix norm while spectral norm is the \\(L_{2}\\) matrix norm."
  },
  {
    "objectID": "posts/2022-03-18-spectral-norm.html#intuition-and-math",
    "href": "posts/2022-03-18-spectral-norm.html#intuition-and-math",
    "title": "Spectral Normalization",
    "section": "Intuition and math",
    "text": "Intuition and math\nSo what does it mean when someone adds “spectral norm” to their deep learning model? It simply means that the weights of the model will be matrices with unit spectral norm. Note that there is no “spectral norm layer” like batch norm or instance norm. Similar to weight normalization, spectral normalization modifies the weights of a linear or convolution layer, instead of its outputs. But what effect does adding spectral normalization actually have on the model? In this section, I will try to explain the intuition behind spectral normalization. Please note that this is only my understanding and I am happy to be corrected if you see any errors.\n\nSingular Value Decomposition\nTo understand what spectral normalization is doing to a weight matrix we need to look at the singular value decomposition of the weight matrix. As a quick overview, SVD allows us to decompose any matrix \\(M\\) into a product of 3 matrices\n\n\n\\[\n    M = U \\cdot \\Sigma \\cdot V^T\n\\]\n\n\nFor a real \\(M\\), the matrices \\(U\\) and \\(V\\) are orthogonal and \\(\\Sigma\\) is a diagonal matrix containing the singular values. The singular values are always \\(\\geq 0\\).\nNow, if \\(M\\) is a weight matrix in a neural network, it can also be thought of as a linear transformation acting on an input feature map. In this case, performing SVD on \\(M\\) allows us to decompose this linear transformation into 3 parts: a rotation, a coordinate scaling and a second rotation. So what happens if we now add the constraint of spectral normalization on \\(M\\)? We divide \\(M\\) by \\(max(\\Sigma)\\) to get a new matrix \\(M_{norm}\\). The singular values of \\(M_{norm}\\) are in range \\([0,1]\\). This means that we have limited the ability of our weight matrix to scale the input feature map, while keeping its ability to rotate it unconstrained.\nThis keeps gradient explosion in check by ensuring each layer scales input feature maps (and gradients in backward pass) by a limited amount. Mathematically this can be expressed by the inequality:\n\n\n\\[\n    \\|M\\cdot x\\| \\leq \\|M\\|_2 \\cdot \\|x\\|\n\\]\n\n\nThat is, the norm of a vector \\(x\\) being acted upon by the matrix \\(M\\) grows by atmost \\(||M||_2\\) times. Since we have limited \\(||M||_2\\) to 1, we have limited the amount by which the norm of \\(x\\) can grow.\nThis effect is illustrated in the following figures. Fig-1 (taken from here) illustrates the action of a matrix \\(M\\) on a unit radius disk.\n\n\n\n\nFig-1: Linear transformation\n\n\n\nLet us take a vector \\(\\vec{x_1}\\) on this disk. Fig-2 shows how this vector gets transformed by M. After being rotated by \\(V^T\\), \\(\\vec{x_1}\\) becomes aligned with the x-axis. This means that \\(\\Sigma\\) stretches its entire magnitude by \\(\\sigma_1\\) since the component of \\(V^T\\vec{x_1}\\) along y-axis is \\(0\\). Since \\(U\\) is another orthogonal matrix that preserves magnitudes, \\(\\vec{x_1}\\) got the maximum possible magnitude increase after being acted upon by matrix \\(M\\), and its magnitude is now \\(\\sigma_1 \\cdot |\\vec{x_1}|\\).\n\n\n\n\nFig-2: Max stretching\n\n\n\nNow let’s prove the inequality mathematically. Let\n\n\n\\[\n    \\vec{x_1} = p\\hat{i} + q\\hat{j}\n\\]\n\n\nwhere \\(\\hat{i}\\) and \\(\\hat{j}\\) are unit vectors along x and y-axes respectively (i.e., they form the orthonormal base for the 2D vector space) and\n\n\n\\[\n    V^T\\vec{x_1} = a\\hat{i} + b\\hat{j}\n\\]\n\n\nSince \\(V\\), and hence \\(V^T\\), is orthogonal\n\n\n$$\n\\[\\begin{aligned}\n\n|V^T\\vec{x_1}| &= |\\vec{x_1}|\\\\\n\n\\Rightarrow \\; a^2 + b^2 &= p^2 + q^2 = C\n\n\\end{aligned}\\]\n$$\n\n\nNow, let \\(f = |\\Sigma V^T\\vec{x_1}| = \\sqrt{\\sigma_1^2a^2 + \\sigma_2^2b^2}\\)\nWe need to maximize \\(f\\) wrt \\(a, b\\)\n\n\n$$\n\\[\\begin{aligned}\n\\underset {a,b}{\\operatorname {arg\\,max} } \\; f^2 &= \\sigma_1^2a^2 + \\sigma_2^2b^2\\\\\n\n&= (\\sigma_1^2 - \\sigma_2^2)a^2 + \\sigma_2^2(a^2 + b^2)\\\\\n\n&= (\\sigma_1^2 - \\sigma_2^2)a^2 + \\sigma_2^2C\n\\end{aligned}\\]\n$$\n\n\nSo, to maximize \\(f\\) we need to maximize \\(a\\)\n\n\n$$\n\\[\\begin{aligned}\n\n& \\underset {a}{\\operatorname {arg\\,max} } \\; a^2 + b^2 = C\\\\\n\\Rightarrow & \\; a = \\sqrt{C}, b=0\n\n\\end{aligned}\\]\n$$\n\n\nTherefore, the maximum value of \\(f = \\sigma_1\\sqrt{C} = \\sigma_1|V^T\\vec{x_1}| = \\sigma_1|\\vec{x_1}|\\)\nFinally, since \\(U\\) is also orthogonal,\n\n\n\\[\n    \\max |U\\Sigma V^T\\vec{x_1}| = \\max |\\Sigma V^T\\vec{x_1}| = \\sigma_1|\\vec{x_1}|\n\\]\n\n\n\n\nLipschitz Continuity\nWhile spectral normalization acts as a normalization method by keeping gradient explosion in check, it also acts as a powerful regularization method. This is because several weight matrices \\(W\\) can map to the same normalized matrix \\(W_{norm}\\). Thus spectral norm constrains the space of weights.\nIn fact, spectral normalization constrains neural networks to model only a specific family of functions called Lipschitz continuous functions. A \\(K\\)-Lipschitz continuous function is a function whose slope between any two points is always \\(\\leq K\\). This \\(K\\) is called the Lipschitz constant of the function. A neural network with spectral normalization always forms a Lipschitz continuous functions because of the following 3 properties: 1. The Lipschitz constant for linear layers is equal to the spectral norm of their weight matrices. 2. The Lipschitz constant of a composition of functions \\(f \\circ g \\leq\\) Lipschitz constant of \\(f \\times\\) Lipschitz constant of \\(g\\) 3. The Lipschitz constant for activation functions like ReLU and pooling layers is \\(1\\).\nIn a neural network with spectral normalization, property 1 ensures that each individual weight layer is \\(1\\)-Lipschitz while properties 2 and 3 ensure that the entire arbitrary sized model is also Lipschitz continuous. Why is it important to model only Lipschitz continuous functions? The use cases are explained in the next section."
  },
  {
    "objectID": "posts/2022-03-18-spectral-norm.html#when-to-use-spectral-normalization",
    "href": "posts/2022-03-18-spectral-norm.html#when-to-use-spectral-normalization",
    "title": "Spectral Normalization",
    "section": "When to use Spectral Normalization?",
    "text": "When to use Spectral Normalization?\nThis is probably the most important part of this post. But unfortunately, like most things in deep learning there is no specific answer to this question. From the intuition above, spectral norm’s purpose can be understood to constrain the weight space of a neural network. It can find a use wherever this property is desired.\n\nGenerative models and reinforcement learning\nThe most common use case for spectral normalization is in GANs. Miyato et al. [1] proved that adding spectral normalization to the layers of a discriminator leads to more stable training of GANs. Here’s how: In GANs, the generator does not directly receve any gradients because we do not have any ground truth for it. The only way it receives gradients is through the discriminator. This is why it is important for the derivative of the discriminator to be bounded, otherwise the generator can become unstable. But the function modelled by a normal neural network doesn’t need to have bounded derivatives.\nEnter Lipschitz continuity. As mentioned before, a \\(K\\)-Lipschitz continuous function is a function whose slope between any 2 points is always \\(\\leq K\\). In other words, these functions have bounded derivatives. Using spectral normalization, we can constrain the discriminator to only model Lipschitz continuous functions.\nThis concept of Lipschitz continuity doesn’t have to be limited to GANs. If some other application requires this constraint, spectral normalization can help. In fact Lipschitz continuity just refers to “smooth” functions. If a task requires a neural network to model only “smooth” functions, spectral norm is useful.\nZhang et al. [2] went beyond [1] and showed that it is useful to apply spectral normalization to generators as well. While there is no mathematical proof behind why the generator should be regularized, the authors hypothesize that spectral normalization prevents the magnitude of weights from growing too large and prevents unusual gradients.\nGogianu et al. [3] show that applying spectral normalization to a Deep-Q-Network improves performance of reinforcement learning. A Q-Network is a neural network that assigns values to all the actions an agent can take while in its current state. It allows the agent to find an optimal action at every step. Turns out applying spectral norm also helps performance here. The authors also point out that unlike the discriminator of GANs, there is no need for a Q-network to be super smooth. They find it is more beneficial to control the smoothness by applying spectral norm to only a few layers of the network.\n\n\nMulti-task Learning\nIn multi-task learning we often have a common body that projects input data to a common latent space, and multiple task-specific heads for task-specific outputs. For these models, it is important for the common latent space to capture information that is useful for all the heads. Otherwise the model won’t do equally well for all the tasks.\nA simple way to prevent one task from dominating over others is to apply weights to the losses so that all the losses have gradients that are similar in magnitude. But, tuning loss weights only affects the initial gradients for each task. The final gradients received by the common body is the sum of gradients backpropagated through each task head. If the weights in any one head are such that they scale their gradient to have a disproportionately large magnitude, this task will dominate the final gradients received by the common body. This can make tuning the loss weights non-intuitive. It is possible for spectral normalization to help in this case. Spectral norm when applied to each of the task-specific heads, prevents the weights in each head from scaling the common latent vector too differently (think of each head to be rotating the latent vector without affecting its magnitude too much). This forces the heads to cooperate and prevents any one head from dominating, since if the spectral norm of weights in one head grows disproportionately, the gradients from that head can potentially have a much larger magnitude, causing it to dominate the total gradient.\n\n\n\n\nFig-3: Multi-task learning\n\n\n\nFig-3 shows a graph from a paper [4] (authored by yours truly) that gives empirical evidence of this phenomenon. Applying spectral norm brings the precision of the 2 different tasks in our model closer together. The dashed lines represent the original pipeline without spectral norm. The solid lines represent our modified training pipeline that uses spectral norm.\n\n\nPossibly Transfer Learning\nThe idea of using spectral norm for transfer learning has sort of been proposed by Dahiya et al. in [5]. It is another paper in the same domain as [4]. While our approach was to learn the two tasks in parallel, creating a multi-task problem, [5] learns the tasks sequentially. For this they needed to make sure that when training the model for the second task, it does not lose performance on the first task. To achieve this, they use spectral norm on their weight layers. [5] proves that using spectral norm prevents the initially trained weights from drifting too far away and the model can do well on both tasks.\n[5] trains a small model with a single weight layer and applies spectral norm to it. There is a possibility that we can scale up this idea of reducing weight drift to larger models as well. It might be possible to take a CNN pretrained on Imagenet and train it on a new dataset while retaining close to the original performance on imagenet. That is, spectral norm might offer a way to reduce catastrophic forgetting.\nTo test this I ran some experiments with the MIT Indoor Scene Recognition dataset [6]. This dataset is sufficiently different from Imagenet. Standard pretrained resnet models are not trained with spectral normalization and simply adding spectral normalization to a pretrained model degrades performance. Training on imagenet with spectral normalization is also not a good idea because spectral normalization decreases classification performance over batch norm (because of overly strong regularization). One solution is to take a normal pretrained model and calculate the initial spectral norm of each weight layer as \\(\\sigma_W\\). Now, instead of limiting this spectral norm to be in range \\([0,1]\\), we can limit it to be in range \\([0, \\sigma_W]\\) ( similar to the approach suggested in [3] and [7] for reducing the strength of regularization of spectral norm). The old batch norm layers can be frozen to act as simple affine layers.\nResults were inconclusive with these experiments. Adding this kind of spectral normalization to a classifier causes it to not train at all for the first few epochs and then improve rapidly to match the performance of standard transfer learning, without spectral normalization."
  },
  {
    "objectID": "posts/2022-03-18-spectral-norm.html#pytorch-implementation",
    "href": "posts/2022-03-18-spectral-norm.html#pytorch-implementation",
    "title": "Spectral Normalization",
    "section": "Pytorch implementation",
    "text": "Pytorch implementation\nNot much is needed to be said here. “Implementing” spectral norm to a model is as simple as changing one line of code.\n\n# from this\nlayer = nn.Linear(100, 100)\n\n# to this\nlayer = torch.nn.utils.spectral_norm(nn.Linear(100, 100))\n\nA small thing to be noted here is how Pytorch implements spectral norm. First the weight matrix is reshaped to 2D. Then the first vectors of both \\(U\\) and \\(V\\) are calculated.\n\n\n\\[\n    \\sigma = u^T \\cdot W \\cdot v\n\\]\n\n\nTo approximate \\(\\vec{u}\\) and \\(\\vec{v}\\) torch uses the power method. Torch initializes both vectors with random values and performs the following two steps \\(n\\)-times:\n\n\n\\[\n    \\begin{aligned}\n        \\vec{u} &= W \\cdot \\vec{v} \\\\\n        \\vec{v} &= W^T \\cdot \\vec{u}\n    \\end{aligned}\n\\]\n\n\nThis calculation happens on every forward pass during train time. During eval, torch uses cached values for \\(\\vec{u}\\) and \\(\\vec{v}\\). Since torch caches the vectors, the state dict of an nn.Module using spectral norm will have two extra keys: ‘weight_u’ and ‘weight_v’. The unnormalized weight is stored with key ‘weight_orig’ instead of ‘weight’ and the normalized weight is calculated on the fly."
  },
  {
    "objectID": "posts/2022-03-18-spectral-norm.html#conclusion",
    "href": "posts/2022-03-18-spectral-norm.html#conclusion",
    "title": "Spectral Normalization",
    "section": "Conclusion",
    "text": "Conclusion\nTo sum up, Spectral Norm is a way to normalize the weights of a deep neural network, instead of its activations. It prevents gradient explosion and, if used on all layers of a network, forces it to model only Lipschitz continuous functions. It can be used for multi-task learning and any other situations where the weights of independent neural networks layers need a “soft tying”.\nThat said, just like most things in deep learning, your mileage will vary. The regularization produced by spectral norm is quite strong and can hurt performance for “standard” tasks like classification and image segmentation. If your task has no precedent of using spectral norm, use it only if you are feeling adventurous."
  },
  {
    "objectID": "posts/2022-03-18-spectral-norm.html#references",
    "href": "posts/2022-03-18-spectral-norm.html#references",
    "title": "Spectral Normalization",
    "section": "References",
    "text": "References\n{% bibliography –cited%}"
  },
  {
    "objectID": "posts/2022-04-23-quantization.html",
    "href": "posts/2022-04-23-quantization.html",
    "title": "Model Quantization",
    "section": "",
    "text": "So you have trained a neural network and want to deploy it. Performance — speed and computational complexity, not just accuracy — matters a lot when in production. If your model can achieve low enough latencies on a cpu instance, you will have a massively lower deployment cost over using a gpu instance. Lower costs equals higher profits.\nModel quantization is (usually) the easiest way to massively speed up your model. If you want to learn more about the theory behind quantization and how it works check out this blogpost. Feeling too lazy to read through all that? Here’s a quick summary. Quantization provides us a way to compress the weights of our model. Weights are usually represented with 32-bit floats. But we “quantize” the weights and reduce this to 8-bits instead. You can go even further and use as less as 1-bit for every parameter, creating binary neural networks, but that is beyond the scope of this post. While quantization directly reduces model size by 4x, that is not the most important part. Using reduced precisions significantly reduces the time taken for matrix multiplication and addition. And I am not talking about measly 10-20% gains either. You can expect a 3-5x speed up when quantizing a model from FP32 to INT8! These gains are serious enough that they offset the performance gap between a CPU and GPU, making real time inference possible on CPU.\nSo… what’s the catch you ask? The catch is that using lower precision arithmetic means there is an increased chance of arithmetic overflow — because we are greatly limiting the range in which values can lie. There are ways to reduce the probability of overflow (more on this later, in calibrate) but the chances still remain."
  },
  {
    "objectID": "posts/2022-04-23-quantization.html#how",
    "href": "posts/2022-04-23-quantization.html#how",
    "title": "Model Quantization",
    "section": "How?",
    "text": "How?\nQuantizing common pytorch models are pretty simple thanks to Pytorch’s quantization API. You need to perform the following steps to get a basic quantized model\n\nStep 0: Create a model\nLet’s create a basic resnet18 model with a binary classification head. Note that we need to use the ‘quantization’ version of resnet18, instead of standard torchvision version. The latter will give an error. I will explain the reason for this later.\n\nresnet = nn.Sequential(\n    *list(models.quantization.resnet18(pretrained=True).children())[:-3], \n    nn.Flatten(), \n    nn.Linear(512,2)\n).cuda()\n\n\n\nStep 1: Fuse layers\nIn this step we will ‘combine’ the layers of our model. This step is actually not related to quantization, but it does give extra speedups.\n\ntorch.quantization.fuse_modules(resnet, [['0', '1', '2']], inplace=True)\nfor i in range(4,8):\n    torch.quantization.fuse_modules(resnet[i][0], \n                                    [['conv1', 'bn1', 'relu'], ['conv2', 'bn2']], \n                                    inplace=True)\n    if resnet[i][0].downsample is not None:\n        torch.quantization.fuse_modules(resnet[i][0].downsample, \n                                        [['0', '1']], \n                                        inplace=True)\n    torch.quantization.fuse_modules(resnet[i][1], \n                                    [['conv1', 'bn1', 'relu'], ['conv2', 'bn2']], \n                                    inplace=True)\n\n\n\nStep 2: Prepare for qat\nPrepare the model for quantization aware training\n\nresnet.qconfig = torch.quantization.get_default_qat_qconfig('fbgemm')\nresnet = torch.quantization.prepare_qat(resnet).cuda()\n\n\n\nStep 3: Train normally\n\n\nStep 4: Post training steps\n\nclass Qresnet(nn.Module):\n    def __init__(self, m):\n        super().__init__()\n        self.q = torch.quantization.QuantStub()\n        self.m = m\n        self.dq = torch.quantization.DeQuantStub()\n    def forward(self, x):\n        return self.dq(self.m(self.q(x)))\n\n# load the best model from training phase\nresnet.load_state_dict(torch.load('best_model.pth'))\n\n# wrap qat resnet with quant dequant stubs\nqmodel = Qresnet(resnet)\n\n# add quantization recipe to model\nqmodel.qconfig = torch.quantization.get_default_qconfig('fbgemm')\n\n# prepare the modules in the model to be quantized\nqmodel = torch.quantization.prepare(qmodel)\n\n# calibrate weights\nfor x,y in train_loader:\n    qmodel(x.cuda())\n\n# actually quantize the trained model. \nqmodel = torch.quantization.convert(qmodel.cpu())\n\n# put to eval mode\nqmodel = qmodel.eval()\n\n# script the model using TorchScript for easy delpoyment\ntorch.jit.script(qmodel)"
  },
  {
    "objectID": "posts/2022-04-23-quantization.html#going-deeper",
    "href": "posts/2022-04-23-quantization.html#going-deeper",
    "title": "Model Quantization",
    "section": "Going deeper",
    "text": "Going deeper\n\nThe quint8 datatype\nWhen pytorch quantizes a 32-bit float tensor it is represented as an 8-bit unsigned int. This is called quint8. You can quantize any fp32 tensor to quint8 using the following command\n\na = torch.randn(10, 10)\naq = torch.quantize_per_tensor(a, scale=0.1, zero_point=10, dtype=torch.quint8)\n\nThe concepts of scale and zero point are explained in the above blogpost. As a short summary, these values determine the range of values in which the quantized tensor can lie. In quantization we achieve performance boosts by significantly limiting the range in which our weights and activations can lie. Unlike an fp32 tensor, the ‘value’ of a quint8 tensor has no meaning. The value along with the scale_factor determines an offset from the fixed zero_point.\n\n\n\n\nStep 0\nIn step:0 we created a ‘quantization’ version of resnet18. This version is exactly same as the original except for the forward function in the Residual blocks. We cannot use the normal version because in resnets there is a skip connection. The final output of resblock is created by adding the inputs with outputs of the conv layers in the block. Standard addition or multiplication is not allowed for tensors of type quint8. This is because we cannot simply add their values without looking at the scale and zero_point.\nThe resulting tensor after any mathematical operation between two quint8 tensors will have a different scale and zero_point. This is taken care of by the nn.quantized.FloatFunctional module. To add two quint8 tensors do the following.\n\nff = nn.quantized.FloatFunctional()\na = torch.tensor(3.0)\nb = torch.tensor(4.0)\nff.add(a, b)  # Equivalent to ``torch.add(a, b)``\n\nThe FloatFunctional module also allows multiplication and relu\n\n\nStep 1\nThis step doesn’t have anything to do with quantization per se. It just reduces the number of operations in your model by combining convolution, batch norm and relu layers. For now only [conv, batch norm] or [conv, batch norm, relu] can be fused. Note that after this step the state_dict of your model will become slightly different, so if you want to load pretrained weights, you should do it before this step.\n\n# before fusing\nresnet[0]\n# out: Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n\n# after fusing\ntorch.quantization.fuse_modules(resnet, [['0', '1', '2']], inplace=False)[0]\n# out: ConvBnReLU2d(\n#   (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n#   (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n#   (2): ReLU()\n# )\n\n\n\nStep 2\nThis step adds a quantization recipe to our model. We are using the ‘fbgemm’ recipe, which seems to be the Pytorch recommended default. The recipe is essentially a strategy for quantizing the model. Different recipes may or may not be able to quantize different operations. As an end user this is all we only need to worry about.\nThe next line actually prepares the model for quantization aware training. The model won’t actually be trained at quint8. The weights and gradients will still be at fp32 but their will be some fake quantization layers visible in the state dict. These are resposnible simulating the effect of precision loss during inference due to quantization. The model will ideally learn to work around the precision loss due to qauntization, and there should be no noticeable drop in performance on deploying this model.\nPytorch calls this fake quantization. Let’s see what happens to the model after this step.\n\n# first conv block after prepare_qat\nresnet[0]\n# out: ConvBnReLU2d(\n#   3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n#   (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n#   (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n#     fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), \n#         scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), \n#         dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n#     (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), \n#         max_val=tensor([], device='cuda:0'))\n#   )\n#   (activation_post_process): FusedMovingAvgObsFakeQuantize(\n#     fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), \n#         scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), \n#         dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n#     (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n#   )\n# )\n\nAs can be seen, some fake quantization ‘stubs’ have been added. These deal with the simulation of precision loss, explained above. Now let’s see what happened to the state_dict.\n\n# state dict of the first conv block after prepare_qat\nresnet[0].state_dict().keys()\n# out: odict_keys(['weight', 'bn.weight', 'bn.bias', 'bn.running_mean', 'bn.running_var', \n# 'bn.num_batches_tracked', 'weight_fake_quant.fake_quant_enabled', 'weight_fake_quant.observer_enabled', \n# 'weight_fake_quant.scale', 'weight_fake_quant.zero_point', 'weight_fake_quant.activation_post_process.eps', \n# 'weight_fake_quant.activation_post_process.min_val', 'weight_fake_quant.activation_post_process.max_val', \n# 'activation_post_process.fake_quant_enabled', 'activation_post_process.observer_enabled', \n# 'activation_post_process.scale', 'activation_post_process.zero_point', \n# 'activation_post_process.activation_post_process.eps', 'activation_post_process.activation_post_process.min_val', \n# 'activation_post_process.activation_post_process.max_val'])\n\nQuite a few new keys have been added for the fake quantization stubs. There is a quantization stub to quantize the weights called weight_fake_quant and another to quantize the activations called activation_post_process. The zero points and scales of these stubs have been added to the state_dict.\n\n# dtype of weight\nresnet[0].weight.dtype\n# out: torch.float32\n\n\n\nStep 4\n\n\n4.1: Wrap\nAfter training run we need to actually convert the model to fp32. The quantized model will take quint8 inputs, so we need to wrap the model with Quantization and DeQuantization stubs. This achived with the new Qresnet class. We need this because we cannot manually quantize our input tensors without knowing the scale and zero point. Using a Quant stub will allow torch to calculate these values using the train set. Note that the DeQuant stub has no parameters.\n\n\n4.2: Prepare\nThis is another prepare step. The point of this step is to add necesary components to the model to allow calculation of zero point and scale for quantization. In our case it does not change the state dict since we have trained the model with qat added fake quantization stubs. These stubs have calculated the necessary scale and zero points.\nBut, if we were training without qat, this step would add a HistogramObserver module\n\n# qmodel.m is a resnet on which prepare_qat has NOT been called\nqmodel.m[0]\n# out: ConvBnReLU2d(\n#   (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n#   (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n#   (2): ReLU()\n#   (activation_post_process): HistogramObserver()\n# )\n\n\n\n4.3 : Calibrate\nIn this step we forward pass all elements of the training set through the model to calibrate the weights. Remember how we mentioned earlier that there is a way to reduce the probability of overflow due to reduced precision in computation? That is what is happening here. We assume that the train set is representative of the actual data distribution. Under this assumption we can calculate the range in which inputs and outputs of each layer lies, and set the scale and zero points accordingly. As long as we do not encounter a strongly out-of-distribution data point at test time, there is a low risk of overflow.\nOf course since we have used qat, these values have already been computed, except for the input quant stub. If you have a very large train set, this step can take quite some time. In this case you might be able to get away with using a subset of your train set.\n\n\n4.4 : Quantize\nAfter this step, the weights have finally turned to quint8 from fp32\n\n# first conv block after quantization\nqmodel.m[0]\n# out: QuantizedConvReLU2d(3, 64, kernel_size=(7, 7), stride=(2, 2), scale=0.03747640550136566, \n# zero_point=0, padding=(3, 3))\n\n\n# weight of first conv block\n# note that weight is now a function instead of parameter\nqmodel.m[0].weight().dtype\n# out: torch.qint8\n\n\n\n4.5: Script\nFinally you can use torchscript to compile your model. This is also an optional step but recommended since if you just save the weights of your quantized model, you will have to do all the steps for creating the quantized model, at inference time, before you can load the weights."
  },
  {
    "objectID": "posts/2022-04-23-quantization.html#the-how",
    "href": "posts/2022-04-23-quantization.html#the-how",
    "title": "Model Quantization",
    "section": "The how",
    "text": "The how\nQuantizing common pytorch models are pretty simple thanks to Pytorch’s quantization API. You need to perform the following steps to get a basic quantized model\n\nStep 0: Create a model\nLet’s create a basic resnet18 model with a binary classification head. Note that we need to use the ‘quantization’ version of resnet18, instead of standard torchvision version. The latter will give an error. I will explain the reason for this later.\n\nimport torchvision.models as models\n\nresnet = nn.Sequential(\n    *list(models.quantization.resnet18(pretrained=True).children())[:-3], \n    nn.Flatten(), \n    nn.Linear(512,2)\n).cuda()\n\n\n\nStep 1: Fuse layers\nIn this step we will ‘combine’ the layers of our model. This step is actually not related to quantization, but it does give extra speedups.\n\ntorch.quantization.fuse_modules(resnet, [['0', '1', '2']], inplace=True)\n# In our sequential model, there are 4x2 resblocks in positions [4,5,6,7]\nfor i in range(4,8):\n    # fuse modules in the first resblock\n    torch.quantization.fuse_modules(resnet[i][0], \n                                    [['conv1', 'bn1', 'relu'], ['conv2', 'bn2']], \n                                    inplace=True)\n    # if this resblock does downsampling, also fuse the skip-connection\n    if resnet[i][0].downsample is not None:\n        torch.quantization.fuse_modules(resnet[i][0].downsample, \n                                        [['0', '1']], \n                                        inplace=True)\n    # fuse modules in the second resblock\n    torch.quantization.fuse_modules(resnet[i][1], \n                                    [['conv1', 'bn1', 'relu'], ['conv2', 'bn2']], \n                                    inplace=True)\n\n\n\nStep 2: Prepare for qat\nPrepare the model for quantization aware training\n\nresnet.qconfig = torch.quantization.get_default_qat_qconfig('fbgemm')\nresnet = torch.quantization.prepare_qat(resnet).cuda()\n\n\n\nStep 3: Train normally\n\nfor _ in range(epochs):\n    for x, y in train_loader:\n        loss = loss_fn(resnet(x.cuda()), y.cuda())\n        opt.zero_grad()\n        loss.backward()\n        opt.step()\n\n\n\nStep 4: Post training steps\nThis is where the magic happens\n\nclass Qresnet(nn.Module):\n    def __init__(self, m):\n        super().__init__()\n        self.q = torch.quantization.QuantStub()\n        self.m = m\n        self.dq = torch.quantization.DeQuantStub()\n    def forward(self, x):\n        return self.dq(self.m(self.q(x)))\n\n# load the best model from training phase\nresnet.load_state_dict(torch.load('best_model.pth'))\n\n# 4.1: wrap qat resnet with quant dequant stubs\nqmodel = Qresnet(resnet)\n\n# add quantization recipe to model again\nqmodel.qconfig = torch.quantization.get_default_qconfig('fbgemm')\n\n# 4.2: prepare the modules in the model to be quantized\nqmodel = torch.quantization.prepare(qmodel)\n\n# 4.3: calibrate weights\nfor x,y in train_loader:\n    qmodel(x.cuda())\n\n# 4.4: actually quantize the trained model. \nqmodel = torch.quantization.convert(qmodel.cpu())\n\n# put to eval mode\nqmodel = qmodel.eval()\n\n# 4.5: script the model using TorchScript for easy delpoyment (optional)\ntorch.jit.script(qmodel)"
  }
]